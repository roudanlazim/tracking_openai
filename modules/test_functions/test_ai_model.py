import sys
import os
import getpass  # âœ… Hide API key input

# âœ… Add project root to `sys.path` so Python can find `modules/`
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))

from modules.ai_model import get_ai_prediction
from modules.system_settings import SystemSettings
from modules.prompt_generator import load_prompt  # âœ… Ensure we load a prompt from JSON

# âœ… Step 1: Ask for API Key (Hidden Input) & Store It
if not SystemSettings.api_key:
    SystemSettings.api_key = getpass.getpass("\nğŸ”‘ Enter your OpenAI API Key: ").strip()

if not SystemSettings.api_key:
    print("âŒ No API Key entered. Exiting test.")
    sys.exit(1)

print(f"âœ… API Key Stored: {SystemSettings.api_key[:6]}********")  # âœ… Masked API key for security

# âœ… Step 2: Define Test Input
test_input_text = "Package delivered to customer"
test_status_elements = ["In Transit", "Delivered", "Exception"]

# âœ… Convert the prompt file path to an absolute path
test_prompt_file = os.path.abspath("data/prompts/example_prompt.json")

# âœ… Debugging: Print the absolute path to confirm location
print(f"ğŸ” Checking prompt file at: {test_prompt_file}")

# âœ… Step 3: Load JSON Prompt
prompt_data = load_prompt(test_prompt_file)
if not prompt_data:
    print(f"âŒ Failed to load prompt file: {test_prompt_file}")
    sys.exit(1)

print(f"âœ… Loaded Prompt from {test_prompt_file}")

### âœ… STEP 4: CALL AI MODEL
print("\nğŸ¤– Testing get_ai_prediction()")
ai_response, token_counts = get_ai_prediction(test_input_text, test_status_elements, test_prompt_file)

### âœ… STEP 5: DISPLAY RESULTS
print(f"\nâœ… AI Response: {ai_response}")
print(f"ğŸ”¢ Token Counts: {token_counts}")

# âœ… Check if AI returned a valid response
if not ai_response or "error" in ai_response.lower():
    print("âŒ AI returned an error! Check API key and model settings.")
else:
    print("âœ… AI model call was successful!")
